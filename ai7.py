"""
Fixed Arabic Voice Assistant - Streamlit Interface
With working voice recording and proper Arabic responses
"""

import streamlit as st
import sounddevice as sd
import numpy as np
import wave
import tempfile
import os
import base64
from pathlib import Path
import json
import time
import threading

# Core imports
try:
    from vosk import Model, KaldiRecognizer
    from gtts import gTTS
    from openai import OpenAI
except ImportError as e:
    st.error(f"Missing package: {e}")
    st.stop()

# Page configuration
st.set_page_config(
    page_title="ğŸ¦· Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†",
    page_icon="ğŸ¦·",
    layout="wide"
)

# Session state initialization
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'assistant_ready' not in st.session_state:
    st.session_state.assistant_ready = False
if 'is_recording' not in st.session_state:
    st.session_state.is_recording = False
if 'audio_data' not in st.session_state:
    st.session_state.audio_data = None
if 'chat_mode' not in st.session_state:
    st.session_state.chat_mode = "written"  # "written", "audio", or "twins"
if 'auto_record' not in st.session_state:
    st.session_state.auto_record = False
if 'first_message_sent' not in st.session_state:
    st.session_state.first_message_sent = False

class ArabicVoiceAssistant:
    def __init__(self):
        """Initialize Arabic Voice Assistant"""
        self.openai_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key="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"
        )
        
        self.model = None
        self.rec = None
        self.sample_rate = 16000
        
        # Pure Arabic system prompt
        self.system_prompt = """Ø£Ù†Øª Ø³Ø§Ù†Ø¯ÙŠØŒ Ù…ÙˆØ¸ÙØ© Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†.

Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©:
- Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„: Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù…Ø¹Ø© Ù…Ù† 8 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 6 Ù…Ø³Ø§Ø¡Ù‹ØŒ Ø§Ù„Ø³Ø¨Øª Ù…Ù† 9 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 3 Ù…Ø³Ø§Ø¡Ù‹
- Ø§Ù„Ù…ÙˆÙ‚Ø¹: ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±
- Ø§Ù„Ù‡Ø§ØªÙ: (604) 555-DENTAL
- Ø§Ù„Ø®Ø¯Ù…Ø§Øª: Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…ØŒ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù†Ø§Ù†ØŒ Ø§Ù„Ø­Ø´ÙˆØ§ØªØŒ Ø§Ù„ØªÙŠØ¬Ø§Ù†ØŒ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±ØŒ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:
1. Ø§Ø¬ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
2. ÙƒÙˆÙ†ÙŠ ÙˆØ¯ÙˆØ¯Ø© ÙˆÙ…Ù‡Ù†ÙŠØ©
3. Ø§Ø¬Ø¹Ù„ÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
4. Ø§Ø³Ø£Ù„ÙŠ Ø¯Ø§Ø¦Ù…Ø§Ù‹ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£ÙƒØ«Ø±
5. Ø¹Ù†Ø¯ Ø­Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ØŒ Ø§Ø·Ù„Ø¨ÙŠ Ø§Ø³Ù… Ø§Ù„Ù…Ø±ÙŠØ¶ ÙˆÙ†ÙˆØ¹ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

Ù…Ø«Ø§Ù„ Ù„Ù„ØªØ±Ø­ÙŠØ¨: "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†. Ø§Ø³Ù…ÙŠ Ø³Ø§Ù†Ø¯ÙŠØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"
"""

    def find_and_load_model(self):
        """Find and load Vosk Arabic model"""
        try:
            search_paths = [Path.cwd(), Path.home() / "Downloads"]
            
            for search_dir in search_paths:
                if not search_dir.exists():
                    continue
                    
                for item in search_dir.iterdir():
                    if (item.is_dir() and 
                        'vosk' in item.name.lower() and 
                        'ar' in item.name.lower()):
                        
                        if (item / 'am').exists() and (item / 'graph').exists():
                            self.model = Model(str(item))
                            self.rec = KaldiRecognizer(self.model, self.sample_rate)
                            return True, f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†: {item.name}"
            
            return False, "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Vosk Ø§Ù„Ø¹Ø±Ø¨ÙŠ"
        
        except Exception as e:
            return False, f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}"

    def transcribe_audio_file(self, audio_file_path):
        """Transcribe audio file using Vosk"""
        if not self.rec:
            return ""
        
        try:
            # Reset recognizer for new audio
            self.rec = KaldiRecognizer(self.model, self.sample_rate)
            
            # Read WAV file
            with wave.open(audio_file_path, 'rb') as wav_file:
                # Check audio format
                if wav_file.getframerate() != self.sample_rate:
                    st.warning(f"âš ï¸ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¹ÙŠÙ†Ø© ØºÙŠØ± Ù…ØªØ·Ø§Ø¨Ù‚: {wav_file.getframerate()} Hz")
                
                # Read audio data
                audio_data = wav_file.readframes(wav_file.getnframes())
                
                # Process with Vosk
                if self.rec.AcceptWaveform(audio_data):
                    result = json.loads(self.rec.Result())
                    text = result.get("text", "").strip()
                else:
                    # Get final result
                    result = json.loads(self.rec.FinalResult())
                    text = result.get("text", "").strip()
                
                return text
        
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ: {e}")
            return ""

    def generate_response(self, user_text):
        """Generate Arabic response"""
        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            
            # Add recent chat history (last 5 messages)
            for msg in st.session_state.chat_history[-5:]:
                messages.append({"role": "user", "content": msg["user"]})
                messages.append({"role": "assistant", "content": msg["assistant"]})
            
            # Add current user message
            messages.append({"role": "user", "content": user_text})
            
            # Generate response
            response = self.openai_client.chat.completions.create(
                model="google/gemma-2-9b-it",
                messages=messages,
                max_tokens=200,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
        
        except Exception as e:
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."

    def generate_greeting(self):
        """Generate a greeting message"""
        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            messages.append({"role": "user", "content": "Ù…Ø±Ø­Ø¨Ø§Ù‹"})
            
            # Generate response
            response = self.openai_client.chat.completions.create(
                model="google/gemma-2-9b-it",
                messages=messages,
                max_tokens=200,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
        
        except Exception as e:
            return "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†. Ø§Ø³Ù…ÙŠ Ø³Ø§Ù†Ø¯ÙŠØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"

    def text_to_speech(self, text):
        """Generate Arabic TTS"""
        try:
            tts = gTTS(text=text, lang='ar', slow=False)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
            tts.save(temp_file.name)
            return temp_file.name
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª: {e}")
            return None

# Initialize assistant
@st.cache_resource
def get_assistant():
    return ArabicVoiceAssistant()

def record_audio(duration=5):
    """Record audio for specified duration"""
    try:
        st.info(f"ğŸ”´ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„... ({duration} Ø«ÙˆØ§Ù†ÙŠ)")
        
        # Record audio
        audio_data = sd.rec(int(duration * 16000), 
                          samplerate=16000, 
                          channels=1, 
                          dtype='float32')
        
        # Show countdown
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i in range(duration):
            progress_bar.progress((i + 1) / duration)
            status_text.text(f"ğŸ¤ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø¬Ø§Ø±ÙŠ... {duration - i} Ø«ÙˆØ§Ù†ÙŠ Ù…ØªØ¨Ù‚ÙŠØ©")
            time.sleep(1)
        
        sd.wait()
        progress_bar.empty()
        status_text.empty()
        
        # Convert to int16 and save as WAV
        audio_int16 = (audio_data * 32767).astype(np.int16)
        
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        with wave.open(temp_file.name, 'wb') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(16000)
            wav_file.writeframes(audio_int16.tobytes())
        
        return temp_file.name
    
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: {e}")
        return None

def play_audio(audio_file):
    """Play audio in browser"""
    if os.path.exists(audio_file):
        with open(audio_file, "rb") as f:
            audio_bytes = f.read()
        audio_b64 = base64.b64encode(audio_bytes).decode()
        
        audio_html = f"""
        <audio controls autoplay style="width: 100%;">
            <source src="data:audio/mp3;base64,{audio_b64}" type="audio/mp3">
            Ø§Ù„Ù…ØªØµÙØ­ Ù„Ø§ ÙŠØ¯Ø¹Ù… ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª
        </audio>
        """
        st.markdown(audio_html, unsafe_allow_html=True)

def send_greeting(assistant):
    """Send greeting message from assistant"""
    if not st.session_state.first_message_sent:
        with st.spinner("ğŸ¤– Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ø¶ÙŠØ±..."):
            greeting = assistant.generate_greeting()
            
            # Add to chat history
            st.session_state.chat_history.append({
                "user": "",
                "assistant": greeting
            })
            
            # Generate TTS for audio and twins modes
            if st.session_state.chat_mode in ["audio", "twins"]:
                with st.spinner("ğŸ”Š Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ­ÙŠØ© Ø¥Ù„Ù‰ ØµÙˆØª..."):
                    audio_file = assistant.text_to_speech(greeting)
                
                if audio_file:
                    # Store audio reference
                    st.session_state["audio_greeting"] = audio_file
                    
                    # Play audio
                    play_audio(audio_file)
        
        st.session_state.first_message_sent = True
        st.rerun()

def main():
    """Main application"""
    
    # Header
    st.title("ğŸ¦· Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†")
    st.markdown("### ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø°ÙƒÙŠØ© - Ø³Ø§Ù†Ø¯ÙŠ")
    st.markdown("**Ù…Ø±Ø­Ø¨Ø§Ù‹ ÙˆØ£Ù‡Ù„Ø§Ù‹ Ø¨ÙƒÙ…! ÙŠÙ…ÙƒÙ†ÙƒÙ… Ø§Ù„ØªØ­Ø¯Ø« Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©**")
    
    # Initialize assistant
    assistant = get_assistant()
    
    # Sidebar
    with st.sidebar:
        st.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        # Load model
        if not st.session_state.assistant_ready:
            with st.spinner("ğŸ”„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬..."):
                success, message = assistant.find_and_load_model()
                st.info(message)
                
                if success:
                    st.session_state.assistant_ready = True
                else:
                    st.error("ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù†Ù…ÙˆØ°Ø¬ Vosk Ø§Ù„Ø¹Ø±Ø¨ÙŠ")
                    st.stop()
        else:
            st.success("âœ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø²")
        
        st.divider()
        
        # Chat mode selection
        st.header("ğŸ’¬ Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        chat_mode = st.radio(
            "Ø§Ø®ØªØ± Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:",
            ["Ù†ØµÙŠ", "ØµÙˆØªÙŠ", "ØªÙˆØ£Ù…"],
            captions=["Ù…Ø­Ø§Ø¯Ø«Ø© Ù†ØµÙŠØ© Ø¹Ø§Ø¯ÙŠØ©", "ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠ", "Ù†Øµ ÙˆØµÙˆØª Ù…Ø¹Ø§Ù‹"],
            index=0
        )
        
        # Map selection to mode
        mode_map = {"Ù†ØµÙŠ": "written", "ØµÙˆØªÙŠ": "audio", "ØªÙˆØ£Ù…": "twins"}
        st.session_state.chat_mode = mode_map[chat_mode]
        
        # Recording settings for audio modes
        if st.session_state.chat_mode in ["audio", "twins"]:
            st.subheader("ğŸ¤ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ³Ø¬ÙŠÙ„")
            record_duration = st.slider("Ù…Ø¯Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Ø«Ø§Ù†ÙŠØ©)", 3, 10, 5)
            
            if st.session_state.chat_mode == "audio":
                st.session_state.auto_record = st.checkbox("Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„Ø±Ø¯", value=True)
        
        st.divider()
        
        # Clear chat
        if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
            st.session_state.chat_history = []
            st.session_state.first_message_sent = False
            st.rerun()
        
        st.divider()
        
        # Clinic info
        st.header("ğŸ¥ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹ÙŠØ§Ø¯Ø©")
        st.info("""
        **Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†**
        
        ğŸ“ ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±  
        ğŸ“ (604) 555-DENTAL
        
        **Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„:**
        â€¢ Ø§Ù„Ø§Ø«Ù†ÙŠÙ†-Ø§Ù„Ø¬Ù…Ø¹Ø©: 8Øµ-6Ù…
        â€¢ Ø§Ù„Ø³Ø¨Øª: 9Øµ-3Ù…  
        â€¢ Ø§Ù„Ø£Ø­Ø¯: Ù…ØºÙ„Ù‚
        
        **Ø§Ù„Ø®Ø¯Ù…Ø§Øª:**
        â€¢ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…
        â€¢ ØªÙ†Ø¸ÙŠÙ ÙˆÙØ­Øµ Ø§Ù„Ø£Ø³Ù†Ø§Ù†
        â€¢ Ø§Ù„Ø­Ø´ÙˆØ§Øª ÙˆØ§Ù„ØªÙŠØ¬Ø§Ù†
        â€¢ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±
        â€¢ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ
        """)

    # Send greeting message if not already sent
    if st.session_state.assistant_ready and not st.session_state.first_message_sent:
        send_greeting(assistant)

    # Main content
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        
        # Display current mode
        mode_display = {
            "written": "ğŸ“ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù†ØµÙŠ",
            "audio": "ğŸ¤ Ø§Ù„Ù†Ù…Ø· Ø§Ù„ØµÙˆØªÙŠ",
            "twins": "ğŸ‘¥ Ø§Ù„Ù†Ù…Ø· Ø§Ù„ØªÙˆØ£Ù… (Ù†ØµÙŠ ÙˆØµÙˆØªÙŠ)"
        }
        st.info(f"**Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø­Ø§Ù„ÙŠ:** {mode_display[st.session_state.chat_mode]}")
        
        # Voice input section for audio and twins modes
        if st.session_state.chat_mode in ["audio", "twins"]:
            st.subheader("ğŸ¤ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ØµÙˆØªÙŠ")
            
            col_rec1, col_rec2 = st.columns(2)
            
            with col_rec1:
                if st.button("ğŸ™ï¸ Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ³Ø¬ÙŠÙ„", disabled=not st.session_state.assistant_ready):
                    audio_file = record_audio(record_duration)
                    if audio_file:
                        st.session_state.last_recording = audio_file
                        st.success("âœ… ØªÙ… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
                        
                        # Play recorded audio
                        st.audio(audio_file, format="audio/wav")
            
            with col_rec2:
                if st.button("ğŸ¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„") and hasattr(st.session_state, 'last_recording'):
                    with st.spinner("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ..."):
                        text = assistant.transcribe_audio_file(st.session_state.last_recording)
                        
                        if text:
                            process_user_input(assistant, text)
                        else:
                            st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙƒÙ„Ø§Ù…. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
            
            # File upload section
            st.subheader("ğŸ“ Ø±ÙØ¹ Ù…Ù„Ù ØµÙˆØªÙŠ")
            uploaded_file = st.file_uploader("Ø§Ø®ØªØ± Ù…Ù„Ù ØµÙˆØªÙŠ (WAV)", type=['wav'], key="audio_uploader")
            
            if uploaded_file and st.button("ğŸ¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹", key="process_uploaded"):
                # Save uploaded file
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
                temp_file.write(uploaded_file.read())
                temp_file.close()
                
                with st.spinner("ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ..."):
                    text = assistant.transcribe_audio_file(temp_file.name)
                    
                    if text:
                        process_user_input(assistant, text)
                    else:
                        st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙƒÙ„Ø§Ù… ÙÙŠ Ø§Ù„Ù…Ù„Ù.")
                
                os.unlink(temp_file.name)
        
        # Text input for written and twins modes
        if st.session_state.chat_mode in ["written", "twins"]:
            st.subheader("âŒ¨ï¸ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†ØµÙŠ")
            user_text = st.text_area("Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§:", height=100, 
                                    placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø¹ÙŠØ§Ø¯Ø©ØŸ", key="text_input")
            
            if st.button("ğŸ“¤ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø©", key="send_text") and user_text.strip():
                process_user_input(assistant, user_text.strip())
        
        # Chat history
        st.divider()
        st.subheader("ğŸ“‹ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        
        for i, msg in enumerate(st.session_state.chat_history):
            if msg["user"]:  # Only show user messages if they exist
                st.markdown(f"**ğŸ‘¤ Ø§Ù„Ù…Ø±ÙŠØ¶:** {msg['user']}")
            st.markdown(f"**ğŸ¤– Ø³Ø§Ù†Ø¯ÙŠ:** {msg['assistant']}")
            
            # Audio playback for audio and twins modes
            if st.session_state.chat_mode in ["audio", "twins"]:
                audio_key = f"audio_{i}" if i > 0 else "audio_greeting"
                if audio_key in st.session_state:
                    audio_file = st.session_state[audio_key]
                    if os.path.exists(audio_file):
                        st.audio(audio_file, format="audio/mp3")
            
            st.divider()

    with col2:
        st.header("ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        st.metric("ğŸ¤– Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "Ø¬Ø§Ù‡Ø²" if st.session_state.assistant_ready else "ØªØ­Ù…ÙŠÙ„")
        st.metric("ğŸ’¬ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„", len(st.session_state.chat_history))
        st.metric("ğŸ¯ Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©", list(mode_display.values())[list(mode_display.keys()).index(st.session_state.chat_mode)])
        
        if st.session_state.chat_history:
            last_msg = st.session_state.chat_history[-1]
            if last_msg["user"]:
                st.text_area("Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø©:", value=last_msg['user'], height=100, disabled=True)
            else:
                st.text_area("Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø©:", value=last_msg['assistant'], height=100, disabled=True)
        
        # Microphone test
        st.divider()
        st.subheader("ğŸ¤ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†")
        if st.button("Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†"):
            test_microphone()

def process_user_input(assistant, text):
    """Process user input and generate response"""
    st.success(f"ğŸ¯ **Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬:** {text}")
    
    # Generate response
    with st.spinner("ğŸ¤– Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø¯..."):
        response = assistant.generate_response(text)
    
    # Add to chat history
    st.session_state.chat_history.append({
        "user": text,
        "assistant": response
    })
    
    # Generate TTS for audio and twins modes
    if st.session_state.chat_mode in ["audio", "twins"]:
        with st.spinner("ğŸ”Š Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±Ø¯ Ø¥Ù„Ù‰ ØµÙˆØª..."):
            audio_file = assistant.text_to_speech(response)
        
        if audio_file:
            # Store audio reference
            msg_index = len(st.session_state.chat_history) - 1
            st.session_state[f"audio_{msg_index}"] = audio_file
            
            # Play audio
            st.success("ğŸ”Š ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¯...")
            time.sleep(8)  # Ensure TTS file is ready
            play_audio(audio_file)
            
            # Auto-record for audio mode
            if st.session_state.chat_mode == "audio" and st.session_state.auto_record:
                st.info("â³ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù‚Ø¨Ù„ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ...")
                time.sleep(3)  # Wait before auto-recording
                st.rerun()
    
    st.rerun()

def test_microphone():
    """Test microphone"""
    try:
        st.info("ğŸ¤ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù„Ù…Ø¯Ø© 3 Ø«ÙˆØ§Ù†...")
        
        audio_data = sd.rec(int(3 * 16000), samplerate=16000, channels=1, dtype='float32')
        
        progress_bar = st.progress(0)
        for i in range(3):
            progress_bar.progress((i + 1) / 3)
            time.sleep(1)
        
        sd.wait()
        progress_bar.empty()
        
        volume = np.abs(audio_data).mean()
        
        if volume > 0.001:
            st.success(f"âœ… Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯! Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙˆØª: {volume:.4f}")
        else:
            st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ØµÙˆØª. ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†.")
            
    except Exception as e:
        st.error(f"âŒ ÙØ´Ù„ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†: {e}")

if __name__ == "__main__":
    main()


#     # 1ï¸âƒ£ Ø§Ø³Ø­Ø¨ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù„ÙŠ Ø¹Ù„Ù‰ GitHub ÙˆØ§Ø¯Ù…Ø¬ÙŠÙ‡Ø§ Ø­ØªÙ‰ Ù„Ùˆ Ù…Ø´ Ù…Ø±ØªØ¨Ø·Ø©
# git pull origin main --allow-unrelated-histories

# # 2ï¸âƒ£ Ù„Ùˆ Ø­ØµÙ„ ØªØ¹Ø§Ø±Ø¶Ø§Øª (conflicts)ØŒ Ø§ÙØªØ­ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ conflict ÙˆØ¹Ø¯Ù„ÙŠÙ‡Ø§ØŒ 
# #    Ø¨Ø¹Ø¯ÙŠÙ† Ø£Ø¶ÙŠÙÙŠ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª:
# git add .

# # 3ï¸âƒ£ Ø§Ø¹Ù…Ù„ÙŠ commit Ø¨Ø¹Ø¯ Ø­Ù„ Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª
# git commit -m "Ø¯Ù…Ø¬ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ù…Ø¹ Ø§Ù„Ø±ÙŠØ¨Ùˆ Ø§Ù„Ø£ØµÙ„ÙŠ"

# # 4ï¸âƒ£ Ø§Ø±ÙØ¹ÙŠ Ù…Ù† Ø¬Ø¯ÙŠØ¯
# git push -u origin main
